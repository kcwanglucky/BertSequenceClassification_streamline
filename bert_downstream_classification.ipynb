{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/kcwanglucky/bert_run_lm_streamline/blob/master/bert_downstream_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "colab_type": "code",
    "id": "u0LtM5xiE7QW",
    "outputId": "ef2bca9c-51ea-4eae-98e3-eb43e820621e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.5.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.18)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
      "Requirement already satisfied: botocore<1.16.0,>=1.15.18 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.18)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.18->boto3->transformers) (2.8.1)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.18->boto3->transformers) (0.15.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DdxaDbH3EB5v"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PPchZSVAFWCY"
   },
   "outputs": [],
   "source": [
    "# 把各組數量大於mineachgroup的及Question長度小於maxlength的取出來\n",
    "# 然後再轉換index，讓被刪除的index不要留空 Output一個更新後的dataframe\n",
    "def filter_toofew_toolong(df_data, mineachgroup, maxlength):\n",
    "    df_data = df_data[~(df_data.question.apply(lambda x : len(x)) > maxlength)]\n",
    "\n",
    "    counts = df_data[\"index\"].value_counts()\n",
    "    idxs = np.array(counts.index)\n",
    "    \n",
    "    # index numbers of groups with count >= mineachgroup\n",
    "    list_idx = [i for i, c in zip(idxs, counts) if c > mineachgroup]\n",
    "\n",
    "    # filter out data with \"index\" in list_idx \n",
    "    df_data = df_data[df_data[\"index\"].isin(list_idx)]\n",
    "    return df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hu__dO3-MW4e"
   },
   "outputs": [],
   "source": [
    "def reindex(df):\n",
    "    index = df['index']\n",
    "    index2label = {idx:val for idx, val in enumerate(index.unique()) }\n",
    "    label2index = {val:idx for idx, val in index2label.items() }\n",
    "    def getindex4label(label):\n",
    "        return label2index[label]\n",
    "    df[\"index\"] = df[\"index\"].apply(getindex4label) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LMkoefVCvhyu"
   },
   "outputs": [],
   "source": [
    "def preprocessing(df, mineachgroup, maxlength):\n",
    "    df = df.loc[:, [\"index\", \"question\"]]       # get 'index' and 'question' coluumn\n",
    "    df = filter_toofew_toolong(df, mineachgroup, maxlength)\n",
    "    df = reindex(df)\n",
    "\n",
    "    num_labels = len(df['index'].value_counts())\n",
    "    print(\"label的數量：{}\".format(num_labels))\n",
    "    return df, num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zc8p1Bo3Mpsn"
   },
   "outputs": [],
   "source": [
    "\"\"\" 從各類別random sample出fraction比例的資料集\n",
    "    data: df data that includes the \"index\" and \"question\" column\n",
    "    fraction: the fraction of data you want to sample (ex: 0.7)\n",
    "\"\"\" \n",
    "def bootstrap(data, fraction):\n",
    "    # This function will be applied on each group of instances of the same\n",
    "    # class in data.\n",
    "    def sampleClass(classgroup):\n",
    "        return classgroup.sample(frac = fraction)\n",
    "\n",
    "    samples = data.groupby('index').apply(sampleClass)\n",
    "    \n",
    "    # If you want an index which is equal to the row in data where the sample came from\n",
    "    # If you don't change it then you'll have a multiindex with level 0\n",
    "    samples.index = samples.index.get_level_values(1)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hoeQ9R0lM8d4"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    實作一個可以用來讀取訓練 / 測試集的 Dataset，此 Dataset 每次將 tsv 裡的一筆成對句子\n",
    "    轉換成 BERT 相容的格式，並回傳 3 個 tensors：\n",
    "    - tokens_tensor：兩個句子合併後的索引序列，包含 [CLS] 與 [SEP]\n",
    "    - segments_tensor：可以用來識別兩個句子界限的 binary tensor\n",
    "    - label_tensor：將分類標籤轉換成類別索引的 tensor, 如果是測試集則回傳 None\n",
    "\"\"\"\n",
    "class OnlineQueryDataset(Dataset):\n",
    "    # mode: in [\"train\", \"test\", \"val\"]\n",
    "    # tokenizer: one of bert tokenizer\n",
    "    # perc: percentage of data to put in training set\n",
    "    # path: if given, then read df from the path(ex training set)\n",
    "    def __init__(self, mode, df, tokenizer, path = None):\n",
    "        assert mode in [\"train\", \"val\", \"test\"]  # 一般訓練你會需要 dev set\n",
    "        self.mode = mode\n",
    "\n",
    "        if path: \n",
    "            self.df = pd.read_csv(path, sep=\"\\t\").fillna(\"\")\n",
    "        else:\n",
    "            self.df = df\n",
    "        self.len = len(self.df)\n",
    "        self.tokenizer = tokenizer \n",
    "    \n",
    "    # 定義回傳一筆訓練 / 測試數據的函式\n",
    "    #@pysnooper.snoop()  # 加入以了解所有轉換過程\n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode == \"test\":\n",
    "            text = self.df.iloc[idx, 1]\n",
    "            label_tensor = None\n",
    "        elif self.mode == \"val\":\n",
    "            label, text = self.df.iloc[idx, :].values\n",
    "            label_tensor = torch.tensor(label)\n",
    "        else:\n",
    "            label, text = self.df.iloc[idx, :].values\n",
    "            # 將label文字也轉換成索引方便轉換成 tensor\n",
    "            label_tensor = torch.tensor(label)\n",
    "        \n",
    "        # create BERT tokens for sentence\n",
    "        word_pieces = [\"[CLS]\"]\n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "        word_pieces += tokens + [\"[SEP]\"]\n",
    "        len_a = len(word_pieces)\n",
    "        \n",
    "        # convert tokens to tokensid\n",
    "        ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
    "        tokens_tensor = torch.tensor(ids)\n",
    "        \n",
    "        # set every non [sep] token to 1, else 0\n",
    "        segments_tensor = torch.tensor([1] * len_a, dtype=torch.long)\n",
    "        \n",
    "        return (tokens_tensor, segments_tensor, label_tensor)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0MnAOwH6tUxn"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "實作可以一次回傳一個 mini-batch 的 DataLoader\n",
    "這個 DataLoader 吃我們上面定義的 OnlineQueryDataset，\n",
    "回傳訓練 BERT 時會需要的 4 個 tensors：\n",
    "- tokens_tensors  : (batch_size, max_seq_len_in_batch)\n",
    "- segments_tensors: (batch_size, max_seq_len_in_batch)\n",
    "- masks_tensors   : (batch_size, max_seq_len_in_batch)\n",
    "- label_ids       : (batch_size)\n",
    "它會對前兩個 tensors 作 zero padding，並產生前面說明過的 masks_tensors\n",
    "\"\"\"\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def create_mini_batch(samples):\n",
    "    tokens_tensors = [s[0] for s in samples]\n",
    "    segments_tensors = [s[1] for s in samples]\n",
    "    \n",
    "    # 訓練集有 labels\n",
    "    if samples[0][2] is not None:\n",
    "        label_ids = torch.stack([s[2] for s in samples])\n",
    "    else:\n",
    "        label_ids = None\n",
    "    \n",
    "    # zero pad 到同一序列長度\n",
    "    tokens_tensors = pad_sequence(tokens_tensors, batch_first=True)\n",
    "    segments_tensors = pad_sequence(segments_tensors, batch_first=True)\n",
    "    \n",
    "    # attention masks，將 tokens_tensors 裡頭不為 zero padding\n",
    "    # 的位置設為 1 讓 BERT 只關注這些位置的 tokens\n",
    "    masks_tensors = torch.zeros(tokens_tensors.shape, dtype=torch.long)\n",
    "    masks_tensors = masks_tensors.masked_fill(tokens_tensors != 0, 1)\n",
    "    \n",
    "    return tokens_tensors, segments_tensors, masks_tensors, label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HAMDFhY1s4eG"
   },
   "outputs": [],
   "source": [
    "\"\"\" 將原本全部的cleaned data依照指定的比例分成train/val/test set，\n",
    "    並output成tsv檔到環境中(檔名ex: 70%train.tsv)\n",
    "    df: df data that includes the \"index\" and \"question\" column\n",
    "    fraction: fraction of all data to be assigned to training set\n",
    "    The remaining (1-fraction) data will be equally splitted between\n",
    "    validation and testing set\n",
    "\"\"\"\n",
    "\n",
    "def output_split(df, fraction = 0.7):\n",
    "    df_train = bootstrap(df, fraction)\n",
    "    df_remain = pd.concat([df_train, df]).drop_duplicates(keep=False)\n",
    "    df_val = df_remain.sample(frac = 0.5)\n",
    "    df_test = pd.concat([df_val, df_remain]).drop_duplicates(keep=False)\n",
    "    del df_remain\n",
    "\n",
    "    print(\"訓練樣本數：\", len(df_train))\n",
    "    print(\"validation樣本數：\", len(df_val))\n",
    "    print(\"預測樣本數：\", len(df_test))\n",
    "    return (df_train, df_val, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ihcil9kJqb6e"
   },
   "outputs": [],
   "source": [
    "def read_online_query(path):\n",
    "    return pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z8TxYfx-1nEw"
   },
   "outputs": [],
   "source": [
    "def getOnlineQueryDataset(mode, df, tokenizer):\n",
    "    return OnlineQueryDataset(mode, df, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ONNepl4hATbo"
   },
   "outputs": [],
   "source": [
    "def get_predictions(model, dataloader, compute_acc=False):\n",
    "    predictions = None\n",
    "    correct = 0\n",
    "    total = 0\n",
    "      \n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            # 將所有 tensors 移到 GPU 上\n",
    "            if next(model.parameters()).is_cuda:\n",
    "                data = [t.to(\"cuda:0\") for t in data if t is not None]\n",
    "            \n",
    "            tokens_tensors, segments_tensors, masks_tensors = data[:3]\n",
    "            outputs = model(input_ids=tokens_tensors, \n",
    "                            token_type_ids=segments_tensors, \n",
    "                            attention_mask=masks_tensors)\n",
    "            \n",
    "            logits = outputs[0]\n",
    "            _, pred = torch.max(logits.data, 1)\n",
    "            \n",
    "            # 用來計算訓練集的分類準確率\n",
    "            if compute_acc:\n",
    "                labels = data[3]\n",
    "                total += labels.size(0)\n",
    "                correct += (pred == labels).sum().item()\n",
    "                \n",
    "            # 將當前 batch 記錄下來\n",
    "            if predictions is None:\n",
    "                predictions = pred\n",
    "            else:\n",
    "                predictions = torch.cat((predictions, pred))\n",
    "    \n",
    "    if compute_acc:\n",
    "        acc = correct / total\n",
    "        return predictions, acc\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yDf9PrcZBYNI"
   },
   "outputs": [],
   "source": [
    "def train(trainloader, valloader, model_name, num_label, epochs):\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        model_name, num_labels=num_label)\n",
    "    clear_output()\n",
    "    \n",
    "    # 讓模型跑在 GPU 上並取得訓練集的分類準確率\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"device:\", device)\n",
    "    model = model.to(device)\n",
    "    pred, acc = get_predictions(model, trainloader, compute_acc=True)\n",
    "    \n",
    "    # 使用 Adam Optim 更新整個分類模型的參數\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        print(\"\")\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch + 1, epochs))\n",
    "        print('Training...')\n",
    "\n",
    "        # 訓練模式\n",
    "        model.train()\n",
    "\n",
    "        for data in trainloader: # trainloader is an iterator over each batch\n",
    "            tokens_tensors, segments_tensors, \\\n",
    "            masks_tensors, labels = [t.to(device) for t in data]\n",
    "\n",
    "            # 將參數梯度歸零\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # forward pass\n",
    "            outputs = model(input_ids=tokens_tensors, \n",
    "                            token_type_ids=segments_tensors, \n",
    "                            attention_mask=masks_tensors, \n",
    "                            labels=labels)\n",
    "\n",
    "            loss = outputs[0]\n",
    "            # backward\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0.\n",
    "            # This is to help prevent the \"exploding gradients\" problem.\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            # 紀錄當前 batch loss\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        # 計算分類準確率\n",
    "        logit, acc = get_predictions(model, trainloader, compute_acc=True)\n",
    "\n",
    "        print('loss: %.3f, acc: %.3f' % (running_loss, acc))    \n",
    "        print(\"\")\n",
    "        print(\"Running Validation...\")\n",
    "\n",
    "        # # Put the model in evaluation mode--the dropout layers behave differently\n",
    "        # # during evaluation.\n",
    "        # model.eval()\n",
    "\n",
    "        # # Evaluate data for one epoch\n",
    "        # for data in valloader:\n",
    "        #     tokens_tensors, segments_tensors, \\\n",
    "        #     masks_tensors, labels = [t.to(device) for t in data]\n",
    "            \n",
    "        #     # Telling the model not to compute or store gradients, saving memory and\n",
    "        #     # speeding up validation\n",
    "        #     with torch.no_grad():\n",
    "        #         # Forward pass, calculate logit predictions.\n",
    "        #         # This will return the logits rather than the loss because we have\n",
    "        #         # not provided labels.\n",
    "        #         # token_type_ids is the same as the \"segment ids\", which \n",
    "        #         # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "        #         outputs = model(input_ids=tokens_tensors, \n",
    "        #                     token_type_ids=segments_tensors, \n",
    "        #                     attention_mask=masks_tensors, \n",
    "        #                     labels=labels)\n",
    "            \n",
    "        #     # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "        #     # values prior to applying an activation function like the softmax.\n",
    "        #     logits = outputs[0]\n",
    "\n",
    "        _, acc = get_predictions(model, valloader, compute_acc=True)\n",
    "        # Move logits and labels to CPU\n",
    "        #logits = logits.detach().cpu().numpy()\n",
    "        #label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences.\n",
    "        # tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "\n",
    "        # Accumulate the total accuracy.\n",
    "        #eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "        # Track the number of batches\n",
    "        #nb_eval_steps += 1\n",
    "\n",
    "        # Report the final accuracy for this validation run.\n",
    "        print(\"  Accuracy: {0:.2f}\".format(acc))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "785ATfgyTz5A"
   },
   "outputs": [],
   "source": [
    "def plain_accuracy(label, pred):\n",
    "    return (label == pred).sum().item()/len(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wLTkOyv0Ed5H"
   },
   "outputs": [],
   "source": [
    "def save_model(output_dir, model, tokenizer):\n",
    "    # Create output directory if needed\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    print(\"Saving model to %s\" % output_dir)\n",
    "    # Save a trained model, configuration and tokenizer using 'save_pretrained()'.\n",
    "    # They can then be reloaded using 'from_pretrained()'\n",
    "    model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "    model_to_save.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "    # Good practice: save your training arguments together with the trained model\n",
    "    torch.save(args, os.path.join(output_dir, 'training_args.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0mvMhSuMcSmf"
   },
   "outputs": [],
   "source": [
    "def write_prediction(output_dir, pred):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    print(\"Saving prediction to %s\" % os.path.join(output_dir, \"prediction.txt\"))\n",
    "    with open(os.path.join(output_dir, \"prediction.txt\"), 'w') as opt:\n",
    "        opt.write('%s' % pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zixv8iqko_1G"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Required parameters\n",
    "    parser.add_argument(\n",
    "        \"--data_path\", default=None, type=str, required=True, help=\"The input training data file (a text file).\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--epoch\", default=30, type=int\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--batch_size\", default=64, type=int\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--min_each_group\", default=3, type=int\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--maxlength\", default=30, type=int\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model_output\", default=None, type=str, required=True, help=\"The directory to save model.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model_start\", default=None, type=str, required=True, help=\"If want to train from existing model\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model_prediction\", default=None, type=str, required=True, help=\"Store the prediction\"\n",
    "    )\n",
    "\n",
    "    df = read_online_query(args.data_path)\n",
    "    df, NUM_LABELS = preprocessing(df, args.min_each_group, args.maxlength)   # preprocessed\n",
    "    \n",
    "    df_train, df_val, df_test = output_split(df, 0.7)\n",
    "\n",
    "    PRETRAINED_MODEL_NAME = \"bert-base-chinese\"\n",
    "    # 取得此預訓練模型所使用的 tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "    clear_output()\n",
    "    \n",
    "    # 初始化一個專門讀取訓練樣本的 Dataset，使用中文 BERT 斷詞\n",
    "    trainset = getOnlineQueryDataset(\"train\", df_train, tokenizer)\n",
    "    valset = getOnlineQueryDataset(\"val\", df_val, tokenizer)\n",
    "    testset = getOnlineQueryDataset(\"test\", df_test, tokenizer)\n",
    "\n",
    "    # 初始化一個每次回傳 64 個訓練樣本的 DataLoader\n",
    "    # 利用 collate_fn 將 list of samples 合併成一個 mini-batch\n",
    "    BATCH_SIZE = args.batch_size\n",
    "    trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True,  \n",
    "                            collate_fn=create_mini_batch)\n",
    "    valloader = DataLoader(valset, batch_size=BATCH_SIZE,  \n",
    "                         collate_fn=create_mini_batch)\n",
    "    testloader = DataLoader(testset, batch_size=BATCH_SIZE, \n",
    "                        collate_fn=create_mini_batch)\n",
    "    \n",
    "    if args.model_start:    # If model_start is provided, then initialize model with the existing model\n",
    "        PRETRAINED_MODEL_NAME = args.model_start\n",
    "    \n",
    "    model = train(trainloader, valloader, PRETRAINED_MODEL_NAME, NUM_LABELS, args.epoch)\n",
    "    save_model(args.model_output, model, tokenizer)\n",
    "    \n",
    "    predictions = get_predictions(model, testloader).detach().cpu().numpy()\n",
    "    write_prediction(args.model_prediction, predictions)\n",
    "\n",
    "    if 'index' in testset.df:      # If we have labels on test set, we can calculate the accuracy\n",
    "        # 用分類模型預測測試集\n",
    "        test_label = testset.df['index']\n",
    "        print(\"Testset accuracy: %f\" % plain_accuracy(test_label, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aQlovhM6LESa"
   },
   "outputs": [],
   "source": [
    "# set the arguments if not using a command line \n",
    "class Args:\n",
    "    data_path = 'intent.csv'\n",
    "    epoch = 3\n",
    "    batch_size = 64\n",
    "    min_each_group = 5\n",
    "    maxlength = 30\n",
    "    model_output = 'model'\n",
    "    model_start = None\n",
    "    model_prediction = \"pred\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "colab_type": "code",
    "id": "1i3ngZhbo_4P",
    "outputId": "f703ed07-5fc7-43ba-ece0-3b82aa262ecc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n",
      "\n",
      "======== Epoch 1 / 3 ========\n",
      "Training...\n",
      "loss: 214.414, acc: 0.057\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.05\n",
      "\n",
      "======== Epoch 2 / 3 ========\n",
      "Training...\n",
      "loss: 204.815, acc: 0.072\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.07\n",
      "\n",
      "======== Epoch 3 / 3 ========\n",
      "Training...\n",
      "loss: 195.053, acc: 0.117\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.09\n",
      "Saving model to model\n",
      "Saving model to pred/prediction.txt\n",
      "Testset accuracy: 0.106870\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    args = Args()\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dmqx437QRl05"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOTVrd2RE92+C933zNtlO89",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "bert_downstream_classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
